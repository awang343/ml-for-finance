{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing Instructions\n",
    "\n",
    "This is a very good article that tells best techniques to preprocess before PCA: \n",
    "https://towardsdatascience.com/pca-a-practical-journey-preprocessing-encoding-and-inspiring-applications-64371cb134a\n",
    "1. Load the Dataset\n",
    "\n",
    "\t•\tUse the pandas library to load the CSV file and set the date column as the index.\n",
    "\t•\tParse the dates to ensure time-series operations can be performed seamlessly.\n",
    "\t•\tMathematical Reasoning:\n",
    "\t•\tRepresent the dataset as a matrix $X$ with dimensions $T \\times N$, where $T$ is the number of time periods (rows) and $N$ is the number of stocks (columns).\n",
    "\t•\tProper indexing ensures that each row corresponds to a time step, allowing for time-series analysis.\n",
    "\n",
    "2. Handle Missing Data\n",
    "\n",
    "\t•\tReplace any missing values in the dataset with the column mean. Missing values can distort PCA if we do not handle them properly.\n",
    "\t•\tFor a column $j$ with missing values, compute its mean: i.e. replace missing values in column $j$ with $\\bar{X}j$, so that:\n",
    "$$\n",
    "\\bar{X}j = \\frac{1}{T} \\sum{i=1}^{T} X_{i,j} \\quad \\text{(excluding missing values)}\n",
    "$$\n",
    "$$\n",
    "X{i,j} = \\bar{X}j \\quad \\text{if } X{i,j} \\text{ is missing.}\n",
    "$$\n",
    "\n",
    "3. Filter Zero-Variance Columns\n",
    "\n",
    "\t•\tIdentify and remove any columns (stocks) with zero variance. This would mean that the column has constant values accross time and do not contribute to PCA or meaningful analysis. If $\\text{Var}(X_j) = 0$, the column is constant and should be removed since it does not contribute to the total variance.\n",
    "\t•\tCalculate the variance of a column $j$:\n",
    "$$\n",
    "\\text{Var}(X_j) = \\frac{1}{T} \\sum_{i=1}^{T} \\big(X_{i,j} - \\bar{X}_j\\big)^2\n",
    "$$\n",
    "\n",
    "4. Normalize the Data\n",
    "\n",
    "\t•\tStandardize the dataset to have zero mean and unit variance. This ensures all columns are on the same scale, which is critical for PCA, as it is sensitive to differences in scale.\n",
    "\t•\tMathematical Reasoning:\n",
    "\t•\tNormalize each column $j$ as follows:\n",
    "$$\n",
    "Z_{i,j} = \\frac{X_{i,j} - \\bar{X}_j}{\\sigma_j}\n",
    "$$\n",
    "where:\n",
    "\t•\t$\\bar{X}_j$ is the mean of column $j$\n",
    "\t•\t$\\sigma_j$ is the standard deviation of column $j$\n",
    "\t•\tThis ensures that:\n",
    "$$\n",
    "\\text{Mean}(Z_j) = 0 \\quad \\text{and} \\quad \\text{Var}(Z_j) = 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (6037, 1201)\n",
      "Preview:\n",
      "               10026     10032     10044     10065     10104     10107  \\\n",
      "date                                                                     \n",
      "2000-01-03  0.012195 -0.017045  0.029762 -0.003724  0.054099 -0.001606   \n",
      "2000-01-04 -0.084337 -0.026734  0.017341 -0.013084 -0.088360 -0.033780   \n",
      "2000-01-05  0.059211 -0.000742  0.000000 -0.007576 -0.052815  0.010544   \n",
      "2000-01-06  0.003106  0.001486 -0.090909  0.000000 -0.058824 -0.033498   \n",
      "2000-01-07  0.003096  0.007418  0.125000  0.003817  0.076823  0.013068   \n",
      "\n",
      "               10138     10145     10200     10207  ...     88664     88912  \\\n",
      "date                                                ...                       \n",
      "2000-01-03 -0.049069 -0.017335 -0.020000  0.046358  ...  0.001164 -0.045983   \n",
      "2000-01-04 -0.030249 -0.017641  0.000000  0.000000  ... -0.081395 -0.004525   \n",
      "2000-01-05 -0.001835 -0.013468 -0.020408  0.012658  ...  0.020253  0.022727   \n",
      "2000-01-06  0.029412  0.019340  0.020833  0.025000  ... -0.009926  0.000000   \n",
      "2000-01-07 -0.007143  0.053571  0.081633 -0.024390  ...  0.000000 -0.040000   \n",
      "\n",
      "               89456     89858     89915     90983     91287     91556  \\\n",
      "date                                                                     \n",
      "2000-01-03 -0.034682  0.019231 -0.007143  0.016949  0.208333 -0.031359   \n",
      "2000-01-04 -0.029940  0.000000 -0.025180 -0.012500 -0.172414 -0.043165   \n",
      "2000-01-05  0.015432  0.042453 -0.003690  0.004219  0.104167  0.022556   \n",
      "2000-01-06 -0.009119  0.049774  0.037037 -0.008403 -0.018868 -0.106618   \n",
      "2000-01-07 -0.016871  0.008621 -0.021429  0.000000  0.076923 -0.012346   \n",
      "\n",
      "               92655     92690  \n",
      "date                            \n",
      "2000-01-03  0.011765  0.007576  \n",
      "2000-01-04 -0.012791 -0.015038  \n",
      "2000-01-05 -0.002356  0.000000  \n",
      "2000-01-06  0.036600  0.022901  \n",
      "2000-01-07  0.117312 -0.007463  \n",
      "\n",
      "[5 rows x 1201 columns]\n",
      "Removing 0 columns with zero variance.\n",
      "Preprocessing complete.\n",
      "Normalized Data (Preview):\n",
      "               10026     10032     10044     10065     10104     10107  \\\n",
      "date                                                                     \n",
      "2000-01-03  0.557034 -0.545982  1.059581 -0.348658  2.248745 -0.113223   \n",
      "2000-01-04 -4.126841 -0.842453  0.607554 -1.152455 -3.732034 -1.785434   \n",
      "2000-01-05  2.838320 -0.047131 -0.023522 -0.679451 -2.239768  0.518261   \n",
      "2000-01-06  0.116023  0.021042 -3.331894 -0.028856 -2.492040 -1.770777   \n",
      "2000-01-07  0.115537  0.202554  4.525495  0.298932  3.202754  0.649443   \n",
      "\n",
      "               10138     10145     10200     10207  ...     88664     88912  \\\n",
      "date                                                ...                       \n",
      "2000-01-03 -2.161566 -0.936868 -0.535854  2.727517  ...  0.025782 -1.259427   \n",
      "2000-01-04 -1.343472 -0.952949 -0.036661 -0.031305  ... -4.166163 -0.138709   \n",
      "2000-01-05 -0.108334 -0.733643 -0.546038  0.721988  ...  0.995028  0.597983   \n",
      "2000-01-06  1.249954  0.990536  0.483324  1.456476  ... -0.537315 -0.016387   \n",
      "2000-01-07 -0.339069  2.789498  2.000873 -1.482785  ... -0.033321 -1.097691   \n",
      "\n",
      "               89456     89858     89915     90983     91287     91556  \\\n",
      "date                                                                     \n",
      "2000-01-03 -1.058051  0.843410 -0.326224  0.631462  4.430171 -1.475001   \n",
      "2000-01-04 -0.916272 -0.030004 -1.063324 -0.511335 -3.718901 -2.013828   \n",
      "2000-01-05  0.440286  1.898083 -0.185114  0.137462  2.200722  0.985685   \n",
      "2000-01-06 -0.293754  2.230580  1.479235 -0.352347 -0.432578 -4.909830   \n",
      "2000-01-07 -0.525528  0.361536 -0.910036 -0.026260  1.617622 -0.607246   \n",
      "\n",
      "               92655     92690  \n",
      "date                            \n",
      "2000-01-03  0.552868  0.404132  \n",
      "2000-01-04 -0.702454 -0.890303  \n",
      "2000-01-05 -0.169009 -0.029522  \n",
      "2000-01-06  1.822453  1.281341  \n",
      "2000-01-07  5.948515 -0.456707  \n",
      "\n",
      "[5 rows x 1201 columns]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"daily_ret_clean.csv\"\n",
    "data = pd.read_csv(file_path, index_col=0, parse_dates=True)\n",
    "\n",
    "# Display basic information\n",
    "print(\"Dataset Shape:\", data.shape)\n",
    "print(\"Preview:\")\n",
    "print(data.head())\n",
    "\n",
    "# Step 1: Handle missing data\n",
    "# Replace missing values with column mean\n",
    "data = data.fillna(data.mean())\n",
    "\n",
    "# Step 2: Filter columns with zero variance\n",
    "zero_variance_columns = data.columns[data.var() == 0]\n",
    "print(f\"Removing {len(zero_variance_columns)} columns with zero variance.\")\n",
    "data = data.drop(columns=zero_variance_columns)\n",
    "\n",
    "# Step 3: Normalize the data\n",
    "scaler = StandardScaler()\n",
    "normalized_data = scaler.fit_transform(data)\n",
    "\n",
    "# Convert normalized data back to a DataFrame\n",
    "normalized_df = pd.DataFrame(normalized_data, index=data.index, columns=data.columns)\n",
    "print(\"Preprocessing complete.\")\n",
    "print(\"Normalized Data (Preview):\")\n",
    "print(normalized_df.head())\n",
    "\n",
    "# Save the preprocessed data (optional)\n",
    "normalized_df.to_csv(\"preprocessed_returns.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
